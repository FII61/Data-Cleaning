# -*- coding: utf-8 -*-
"""Machine Learning.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1w1r_xhvyE53SriHDeKB-9OzvHLAuqk4n
"""

import csv
import string
import pandas as pd
import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt
from google.colab import drive
import matplotlib.pyplot as plt
from keras.preprocessing.text import Tokenizer 
from keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Model
from sklearn.model_selection import train_test_split
drive.mount('/content/drive')

from keras.layers import Attention

DATAPATH = '/content/drive/MyDrive/Data/'

news_dataset = pd.read_csv(DATAPATH+'combinednews.csv')

news_dataset

news_dataset = news_dataset.drop(columns=['title'])

for x in range(len(news_dataset)):
  news_dataset['combined'][x] = news_dataset['combined'][x].replace('\n',' ')
  news_dataset['combined'][x] = news_dataset['combined'][x].lower()
  news_dataset['combined'][x] = news_dataset['combined'][x].replace('\r','')
  news_dataset['seperated'][x] = news_dataset['seperated'][x].replace('\n',' ')
  news_dataset['seperated'][x] = news_dataset['seperated'][x].lower()
  news_dataset['seperated'][x] = news_dataset['seperated'][x].replace('\r','')

combined_word_count = []
seperated_word_count = []

for i in news_dataset['combined']:
      combined_word_count.append(len(i.split()))

for i in news_dataset['seperated']:
      seperated_word_count.append(len(i.split()))

length_df = pd.DataFrame({'combined':combined_word_count, 'seperated':seperated_word_count})

length_df.hist(bins = 30)
plt.show()

max(combined_word_count),max(seperated_word_count)

max_com = 1870
max_sep = 1570

for x in range(len(news_dataset)):
  print(str(combined_word_count[x])+' '+str(seperated_word_count[x]))

news_dataset['seperated'] = news_dataset['seperated'].apply(lambda x : 'hajime '+ x +' owari')

news_dataset['seperated']

train_data,test_data,train_label,test_label=train_test_split(np.array(news_dataset['combined']),np.array(news_dataset['seperated']),test_size=0.2,random_state=0,shuffle=True)

all_data = np.concatenate((train_data,test_data))
all_label = np.concatenate((train_label,test_label))

# COMBINED TOKENIZING
combined_tokenizer = Tokenizer()
combined_tokenizer.fit_on_texts(list(all_data))
combined_total_words = len(combined_tokenizer.word_index) + 1
print(f'word index dictionary: {combined_tokenizer.word_index}')
print(f'total words: {combined_total_words}')

train_data = combined_tokenizer.texts_to_sequences(train_data)
test_data = combined_tokenizer.texts_to_sequences(test_data)

train_data = pad_sequences(train_data,maxlen=max_com, padding='post') 
test_data = pad_sequences(test_data,maxlen=max_com, padding='post')

# SEPERATED TOKENIZING
seperated_tokenizer = Tokenizer()
seperated_tokenizer.fit_on_texts(list(all_label))
seperated_total_words = len(seperated_tokenizer.word_index) + 1
print(f'word index dictionary: {seperated_tokenizer.word_index}')
print(f'total words: {seperated_total_words}')

train_label = seperated_tokenizer.texts_to_sequences(train_label)
test_label = seperated_tokenizer.texts_to_sequences(test_label)

train_label = pad_sequences(train_label,maxlen=max_sep, padding='post') 
test_label = pad_sequences(test_label,maxlen=max_sep, padding='post')

#MODEL BUILDING
enc_dim = 100
#encoder
en_input = tf.keras.layers.Input(shape=(max_com,))
en_emb = tf.keras.layers.Embedding(combined_total_words, enc_dim,trainable=True)(en_input)

en_lstm1 = tf.keras.layers.LSTM(enc_dim, return_state=True, return_sequences=True)
en_out1, state_h1, state_c1 = en_lstm1(en_emb)

en_lstm2 = tf.keras.layers.LSTM(enc_dim, return_state=True, return_sequences=True)
en_out2, state_h2, state_c2 = en_lstm2(en_out1)

en_lstm3 = tf.keras.layers.LSTM(enc_dim, return_state=True, return_sequences=True)
en_out3, state_h3, state_c3 = en_lstm3(en_out2)

#decoder
dec_input = tf.keras.layers.Input(shape=(None,))
dec_emb = tf.keras.layers.Embedding(seperated_total_words, enc_dim,trainable=True)
dec_emb_layer = dec_emb(dec_input) 
dec_lstm = tf.keras.layers.LSTM(enc_dim, return_state=True, return_sequences=True)
dec_outputs,dec_fwd_state, decoder_back_state = dec_lstm(dec_emb_layer,initial_state=[state_h3, state_c3])

#attention = tf.keras.layers.Attention()([en_out3, dec_outputs])
decoder_dense = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(seperated_total_words, activation='softmax'))
decoder_outputs = decoder_dense(dec_outputs)

model = Model([en_input, dec_input], decoder_outputs) 
model.summary()

model.compile(optimizer='rmsprop',metrics=['accuracy'], loss='sparse_categorical_crossentropy')

es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=2)

history = model.fit(
    [train_data, train_label[:, :-1]],
    train_label.reshape(train_label.shape[0], train_label.shape[1], 1)[:, 1:],
    epochs=50,
    callbacks=[es],
    batch_size=10,
    validation_data=([test_data, test_label[:, :-1]],
                     test_label.reshape(test_label.shape[0], test_label.shape[1], 1)[:, 1:]),
    )